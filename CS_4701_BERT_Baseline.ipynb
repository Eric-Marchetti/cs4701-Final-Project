{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets\n",
        "! pip install datasets"
      ],
      "metadata": {
        "id": "nmEF69AXazKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import create_optimizer\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "CIultU5ZbC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in dataset"
      ],
      "metadata": {
        "id": "iNGQ7lhyawn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AmKHU69XoCd"
      },
      "outputs": [],
      "source": [
        "# Process data so that it can be appropriately used, meaning we will use the\n",
        "# BERT text embedding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "train_ds = Dataset.from_pandas(train[:int(np.ceil(0.8 * len(train)))])\n",
        "val_ds = Dataset.from_pandas(train[int(np.ceil(0.8 * len(train))):])\n",
        "test_ds = Dataset.from_pandas(test)\n",
        "\n",
        "def tokens(ds):\n",
        "    return tokenizer(ds[\"text\"], truncation=True)\n",
        "tokenized_train = train_ds.map(tokens, batched=True)\n",
        "tokenized_val = val_ds.map(tokens, batched=True)\n",
        "tokenized_test = test_ds.map(tokens, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training hyperparameters\n",
        "init_lr=1e-5\n",
        "batch_size = 2\n",
        "num_epochs = 3\n",
        "batches_per_epoch = len(tokenized_train) // batch_size\n",
        "train_steps = int(batches_per_epoch * num_epochs)\n",
        "# train_steps = int(len(tokenized_train)/batch_size * num_epochs)\n",
        "optimizer, scheduler = create_optimizer(init_lr=init_lr, num_warmup_steps=0, num_train_steps=train_steps)\n",
        "\n",
        "\n",
        "# Instantiate our model for text classification\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "\"bert-base-uncased\", num_labels=28\n",
        ")\n",
        "\n",
        "# Prepare the training and validation data and pass our Adam optimizer to the\n",
        "# BERT model\n",
        "train_set = model.prepare_tf_dataset(\n",
        "    tokenized_train,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "val_set = model.prepare_tf_dataset(\n",
        "    tokenized_val,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "test_set = model.prepare_tf_dataset(\n",
        "    tokenized_test[\"text\"],\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "rv-M3YncangV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(x=train_set, epochs = num_epochs)"
      ],
      "metadata": {
        "id": "7Exq6-F_cH0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain validation predictions\n",
        "preds = model.predict(val_set)[\"logits\"]\n",
        "class_preds = np.argmax(preds, axis=1)"
      ],
      "metadata": {
        "id": "g7AOAT7TcLfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(yTv,class_preds)"
      ],
      "metadata": {
        "id": "amBFEbQVcOH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain prediction logits\n",
        "final_preds = model.predict(test_set)[\"logits\"]"
      ],
      "metadata": {
        "id": "m3Qbt-jWcOtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain predictions\n",
        "final_class_preds = np.argmax(final_preds, axis=1)"
      ],
      "metadata": {
        "id": "PL0roaPvcSBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation score performance\n",
        "val_score = accuracy_score(yTv,class_preds)"
      ],
      "metadata": {
        "id": "WTc9iS2zcu1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}